{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M52QDmyzhh9s"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open('data/ass2.pickle', 'rb') as handle:\n",
        "    data = pd.read_pickle(handle)\n",
        "\n",
        "X_train, y_train = data['train']\n",
        "X_dev, y_dev = data['dev']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(f'Number of Rows: {len(y_train)}')\n",
        "print(f'The Classes are: {np.unique(y_train)}')\n",
        "print(f'Class 0: {np.count_nonzero(y_train == 0)}')\n",
        "print(f'Class 1: {np.count_nonzero(y_train == 1)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the above check we understand that we have a binary classification problem so we will focus on algorithms best suited for binary classification.\n",
        "We can see that the dataset is unbalanced so we will balance it.\n",
        "We can also see that we are dealing with a binary classification problem so we will be using algorithms that are a better fit for this kind of problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "# clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
        "# models,predictions = clf.fit(X_train, X_dev, y_train, y_dev)\n",
        "\n",
        "# print(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running LazyPredict we can focus on the few best algorithms we got from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "rus = RandomUnderSampler(random_state=42, replacement=True)\n",
        "x_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
        "x_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f'Under sampled: {len(y_rus)}')\n",
        "print(f'Over sampled: {len(y_ros)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "pca = PCA(n_components=5)\n",
        "pca.fit(X_train)\n",
        "X_train_pca = pca.transform(X_train)\n",
        "\n",
        "anova_filter = SelectKBest(score_func=f_regression, k=5)  \n",
        "anova_filter.fit(X_train, y_train)\n",
        "X_train_skb = anova_filter.transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # import libraries\n",
        "# import seaborn as sns\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# # create pandas df\n",
        "# X = pd.DataFrame(X_train_pca)\n",
        "# y = np.array(y_train)\n",
        "# X['target'] = pd.DataFrame(y.reshape(-1, 1), columns=[\"target\"])\n",
        "# X.head(5)\n",
        "\n",
        "# # check the null values\n",
        "# X.isnull().sum()\n",
        "\n",
        "# # pairplot for distribution\n",
        "# sns.pairplot(X ,hue=\"target\", palette='Set1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the above check we understand that we have a binary classification problem so we will focus on algorithms best suited for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def cross_validation(clf, X, y, k):\n",
        "    sc = StandardScaler()\n",
        "    mean_accuracy = 0\n",
        "    kf = KFold(n_splits=k, shuffle=True)\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        clf.fit(sc.fit_transform(X_train), y_train)\n",
        "        mean_accuracy += accuracy_score(y_test, clf.predict(sc.transform(X_test)))\n",
        "    return mean_accuracy / k\n",
        "\n",
        "def fit_predict(classifier, X_train, y_train):\n",
        "    sc = StandardScaler()\n",
        "    scaled_X_train = sc.fit_transform(X_train)\n",
        "    classifier.fit(scaled_X_train, y_train)\n",
        "\n",
        "    train_score = classifier.score(scaled_X_train, y_train)\n",
        "    dev_score = classifier.score(sc.transform(X_dev), y_dev)\n",
        "    over_fitting = train_score - dev_score\n",
        "\n",
        "    print(f'train score: {round(train_score, 3)} test score {round(dev_score, 3)} overfit {round(over_fitting, 3)}')\n",
        "    return classifier\n",
        "\n",
        "def fit_predict_poly(classifier):\n",
        "    transformed_train = PolynomialFeatures(2).fit_transform(X_train)\n",
        "    transformed_dev = PolynomialFeatures(2).fit_transform(X_dev)\n",
        "    classifier.fit(transformed_train, y_train)\n",
        "\n",
        "    train_score = classifier.score(transformed_train, y_train)\n",
        "    dev_score = classifier.score(transformed_dev, y_dev)\n",
        "    over_fitting = train_score - dev_score\n",
        "\n",
        "    return f'train score: {round(train_score, 3)} test score {round(dev_score, 3)} overfit {round(over_fitting)}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:\n",
            "XGBoost: 0.858968039037406\n",
            "Light GBM: 0.865172008020855\n",
            "Undersampled:\n",
            "XGBoost: 0.8442609310382152\n",
            "Light GBM: 0.8440068212875942\n",
            "Oversampled:\n",
            "XGBoost: 0.8879449838187702\n",
            "Light GBM: 0.865898058252427\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, VotingClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "def check_models(X_train, y_train, k):\n",
        "    print(f'Naive bayes: {cross_validation(GaussianNB(), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'Random Forest: {cross_validation(RandomForestClassifier(random_state = 42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'SVM: {cross_validation(SVC(random_state = 42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'AdaBoost: {cross_validation(AdaBoostClassifier(DecisionTreeClassifier(min_samples_split=10, max_depth=4), n_estimators=10, learning_rate=0.6, random_state=42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'Histogram Gradient Boosting: {cross_validation(HistGradientBoostingClassifier(), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    evc = VotingClassifier(estimators=[('svm',SVC(random_state = 0)),('rf',RandomForestClassifier(random_state = 42)),('hgb',HistGradientBoostingClassifier())],voting='hard')\n",
        "    print(f'Voting Classifier: {cross_validation(evc, X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'Bagging Classifier: {cross_validation(BaggingClassifier(base_estimator = SVC(), random_state = 42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'XGBoost: {cross_validation(XGBClassifier(random_state = 42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'Light GBM: {cross_validation(LGBMClassifier(random_state = 42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "\n",
        "\n",
        "print('Original Data:')\n",
        "check_models(X_train, y_train, 3)\n",
        "print('Undersampled Data:')\n",
        "check_models(x_rus, y_rus, 3)\n",
        "print('Oversampled Data:')\n",
        "check_models(x_ros, y_ros, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_validation(RandomForestClassifier(random_state = 42), x_ros.to_numpy(), y_ros.to_numpy(), 5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "random_forest_classification.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
