{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M52QDmyzhh9s"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open('data/ass2.pickle', 'rb') as handle:\n",
        "    data = pd.read_pickle(handle)\n",
        "\n",
        "X_train, y_train = data['train']\n",
        "X_dev, y_dev = data['dev']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print('Train data:')\n",
        "print(f'Number of Rows: {len(y_train)}')\n",
        "print(f'The Classes are: {np.unique(y_train)}')\n",
        "print(f'Class 0: {np.count_nonzero(y_train == 0)}')\n",
        "print(f'Class 1: {np.count_nonzero(y_train == 1)}')\n",
        "\n",
        "print('Dev data:')\n",
        "print(f'Number of Rows: {len(y_dev)}')\n",
        "print(f'The Classes are: {np.unique(y_dev)}')\n",
        "print(f'Class 0: {np.count_nonzero(y_dev == 0)}')\n",
        "print(f'Class 1: {np.count_nonzero(y_dev == 1)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the dataset is unbalanced so we will balance it.\n",
        "We can also see that we are dealing with a binary classification problem so we will be using algorithms that are a better fit for this kind of problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First we run LazyPredict to choose the few best algorithms to continue with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
        "models,predictions = clf.fit(X_train, X_dev, y_train, y_dev)\n",
        "\n",
        "print(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Balance the data in two ways, Over-Sampling and Under-Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following block needs to be run twice to work sometimes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "rus = RandomUnderSampler(random_state=42, replacement=True)\n",
        "x_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
        "x_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f'Under sampled: {len(y_rus)}')\n",
        "print(f'Over sampled: {len(y_ros)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our data has 14 features, we might need to reduce some of them based on their importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "def pca(k, X):\n",
        "    pca = PCA(n_components=k)\n",
        "    pca.fit(X)\n",
        "    return (pca.transform(X), pca.transform(X_dev))\n",
        "\n",
        "def select_k_best(k, X, y):\n",
        "    k_best = SelectKBest(score_func=chi2, k=k)  \n",
        "    k_best.fit(X, y)\n",
        "    return (k_best.transform(X), k_best.transform(X_dev))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot the data in case we can see some important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# create pandas df\n",
        "X = pd.DataFrame(X_train)\n",
        "y = np.array(y_train)\n",
        "X['target'] = pd.DataFrame(y.reshape(-1, 1), columns=[\"target\"])\n",
        "X.head(5)\n",
        "\n",
        "# check the null values\n",
        "X.isnull().sum()\n",
        "\n",
        "# pairplot for distribution\n",
        "sns.pairplot(X ,hue=\"target\", palette='Set1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def cross_validation(clf, X, y, k):\n",
        "    sc = StandardScaler()\n",
        "    return round(cross_val_score(clf, sc.fit_transform(X), y, cv=k).mean(), 3)\n",
        "\n",
        "def fit_predict(classifier, X_train, y_train):\n",
        "    sc = StandardScaler()\n",
        "    scaled_X_train = sc.fit_transform(X_train)\n",
        "    classifier.fit(scaled_X_train, y_train)\n",
        "\n",
        "    train_score = accuracy_score(classifier.predict(scaled_X_train), y_train)\n",
        "    dev_score = accuracy_score(classifier.predict(sc.transform(X_dev)), y_dev)\n",
        "    over_fitting = train_score - dev_score\n",
        "\n",
        "    return f'train score: {round(train_score, 3)} test score {round(dev_score, 3)} overfit {round(over_fitting, 3)}'\n",
        "\n",
        "def fit_predict_poly(classifier, X_train, y_train, X_dev, y_dev):\n",
        "    transformed_train = PolynomialFeatures(2).fit_transform(X_train)\n",
        "    transformed_dev = PolynomialFeatures(2).fit_transform(X_dev)\n",
        "    sc = StandardScaler()\n",
        "    scaled_X_train = sc.fit_transform(transformed_train)\n",
        "\n",
        "    classifier.fit(scaled_X_train, y_train)\n",
        "\n",
        "    train_score = accuracy_score(classifier.predict(scaled_X_train), y_train)\n",
        "    dev_score = accuracy_score(classifier.predict(sc.transform(transformed_dev)), y_dev)\n",
        "    over_fitting = train_score - dev_score\n",
        "\n",
        "    return f'train score: {round(train_score, 3)} test score {round(dev_score, 3)} overfit {round(over_fitting, 3)}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial test of the best algoriths usin cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, VotingClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "def check_models(X, y, k):\n",
        "    for clf in clfs:\n",
        "        print(clf)\n",
        "        for i in range(1,15):\n",
        "            X_train, X_dev = select_k_best(i, X, y)\n",
        "            print(f'Iteration {i}: cv {cross_validation(clfs[clf], np.array(X_train), np.array(y), k)} Polynomial {fit_predict_poly(clfs[clf], X_train, y, X_dev, y_dev)}')\n",
        "\n",
        "clfs = {\n",
        "    'Random Forest':RandomForestClassifier(random_state = 42),\n",
        "    'SVC':SVC(random_state = 42),\n",
        "    'AdaBoost':AdaBoostClassifier(random_state=42),\n",
        "    'Histogram Gradient Boosting':HistGradientBoostingClassifier(random_state = 42),\n",
        "    'Bagging Classifier':BaggingClassifier(base_estimator = SVC()),\n",
        "    'XGBoost':XGBClassifier(random_state = 42),\n",
        "    'Light GBM':LGBMClassifier(random_state = 42)}\n",
        "\n",
        "print('Regular Data:')\n",
        "check_models(X_train, y_train, 5)\n",
        "print('Undersampled Data:')\n",
        "check_models(x_rus, y_rus, 5)\n",
        "print('Oversampled Data:')\n",
        "check_models(x_ros, y_ros, 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find best hyperparameters for each model using RandomizedSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['log2', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "rf_grid =  {'n_estimators': n_estimators,\n",
        "            'max_features': max_features,\n",
        "            'max_depth': max_depth,\n",
        "            'min_samples_split': min_samples_split,\n",
        "            'min_samples_leaf': min_samples_leaf,\n",
        "            'bootstrap': bootstrap}\n",
        "\n",
        "rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(random_state=42), param_distributions = rf_grid, scoring = 'roc_auc', n_iter = 5, cv = 3, verbose=3, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(x_ros, y_ros)\n",
        "print(rf_random.best_params_)\n",
        "print(rf_random.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "svc_grid = {'C': [0.1, 1, 10, 100],\n",
        "            'gamma': [1, 0.1, 0.01, 0.001],\n",
        "            'kernel': ['rbf', 'poly', 'linear']}\n",
        "svc_random = RandomizedSearchCV(estimator = SVC(), param_distributions = svc_grid, scoring = 'roc_auc', n_iter = 5, cv = 3, verbose=3, random_state=42, n_jobs = -1)\n",
        "svc_random.fit(x_ros, y_ros)\n",
        "print(svc_random.best_params_)\n",
        "print(svc_random.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "xgb_grid = {\n",
        "    \"learning_rate\"     : [0.05, 0.1, 0.15, 0.2, 0.25, 0.3] ,\n",
        "    \"max_depth\"         : [3, 4, 5, 6, 8, 10, 12, 15],\n",
        "    \"min_child_weight\"  : [1, 3, 5, 7],\n",
        "    \"gamma\"             : [0.0, 0.1, 0.2, 0.3, 0.4],\n",
        "    \"colsample_bytree\"  : [0.3, 0.4, 0.5, 0.7]\n",
        "}\n",
        "\n",
        "xgb_random = RandomizedSearchCV(XGBClassifier(random_state=42), param_distributions=xgb_grid, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, verbose=3)\n",
        "xgb_random.fit(x_ros, y_ros)\n",
        "print(xgb_random.best_params_)\n",
        "print(xgb_random.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform as sp_uniform\n",
        "\n",
        "lgbm_grid = {'num_leaves': sp_randint(6, 50), \n",
        "             'min_child_samples': sp_randint(100, 500), \n",
        "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
        "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
        "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
        "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
        "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
        "\n",
        "lgbm_random = RandomizedSearchCV(LGBMClassifier(random_state=42), param_distributions=lgbm_grid, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, verbose=3)\n",
        "\n",
        "lgbm_random.fit(x_ros, y_ros)\n",
        "print(lgbm_random.best_params_)\n",
        "print(lgbm_random.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "hgb_grid = {\n",
        "    'learning_rate': (0.01, 0.1, 1, 10),\n",
        "    'max_leaf_nodes': (3, 10, 30, 60),\n",
        "    'min_samples_leaf': (5, 10, 20, 40),\n",
        "    'max_depth': (5, 6, 7, 8)}\n",
        "\n",
        "hgb_random = RandomizedSearchCV(HistGradientBoostingClassifier(random_state=42), param_distributions=hgb_grid, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, verbose=3)\n",
        "\n",
        "hgb_random.fit(x_ros, y_ros)\n",
        "print(hgb_random.best_params_)\n",
        "print(hgb_random.best_estimator_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the new models with the 'dev' data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_ros = XGBClassifier(\n",
        "        base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "        colsample_bynode=1, colsample_bytree=0.5, gamma=0.4, gpu_id=-1,\n",
        "        importance_type='gain', interaction_constraints='',\n",
        "        learning_rate=0.3, max_delta_step=0, max_depth=15,\n",
        "        min_child_weight=1, monotone_constraints='()',\n",
        "        n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=42,\n",
        "        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
        "        tree_method='exact', validate_parameters=1, verbosity=None)\n",
        "\n",
        "lgbm_ros = LGBMClassifier(\n",
        "        colsample_bytree=0.8027564890191548, min_child_samples=206,\n",
        "        num_leaves=49, random_state=42, reg_alpha=0, reg_lambda=0.1,\n",
        "        subsample=0.7676961204705843)\n",
        "\n",
        "hgb_ros = HistGradientBoostingClassifier(\n",
        "        max_depth=8, max_leaf_nodes=30,\n",
        "        min_samples_leaf=5, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=120)\n",
        "\n",
        "evc_ros1 = VotingClassifier(estimators=[('xgb',xgb_ros), ('hgb', hgb_ros)], voting='soft', flatten_transform=False)\n",
        "evc_ros2 = VotingClassifier(estimators=[('xgb',xgb_ros), ('hgb', hgb_ros), ('rf', rf)], voting='soft', flatten_transform=False)\n",
        "evc_ros3 = VotingClassifier(estimators=[('xgb',xgb_ros), ('hgb', hgb_ros), ('lgbm', lgbm_ros)], voting='soft', flatten_transform=False)\n",
        "evc_ros4 = VotingClassifier(estimators=[('xgb',xgb_ros), ('hgb', hgb_ros), ('lgbm', lgbm_ros), ('rf', rf)], voting='soft', flatten_transform=False)\n",
        "\n",
        "\n",
        "\n",
        "clfs = {'xgb_ros':xgb_ros, 'lgbm_ros':lgbm_ros, 'hgb_ros':hgb_ros, 'rf':rf, 'evc_ros1': evc_ros1, 'evc_ros2':evc_ros2, 'evc_ros3':evc_ros3, 'evc_ros4':evc_ros4}\n",
        "\n",
        "for clf in clfs:\n",
        "        print(f'{clf}: {fit_predict(clfs[clf], x_ros, y_ros)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_reg = XGBClassifier(\n",
        "    base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "    colsample_bynode=1, colsample_bytree=0.5, gamma=0.4, gpu_id=-1,\n",
        "    importance_type='gain', interaction_constraints='',\n",
        "    learning_rate=0.15, max_delta_step=0, max_depth=4,\n",
        "    min_child_weight=1, monotone_constraints='()',\n",
        "    n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=42,\n",
        "    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
        "    tree_method='exact', validate_parameters=1, verbosity=None)\n",
        "\n",
        "lgbm_reg = LGBMClassifier(\n",
        "    colsample_bytree=0.8104721917981516, min_child_samples=118,\n",
        "    min_child_weight=1, num_leaves=30, random_state=42, reg_alpha=0,\n",
        "    reg_lambda=0.1, subsample=0.8243603847850702)\n",
        "\n",
        "hgb_reg = HistGradientBoostingClassifier(\n",
        "    max_depth=5, max_leaf_nodes=30,\n",
        "    min_samples_leaf=5, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=120)\n",
        "\n",
        "evc_reg1 = VotingClassifier(estimators=[('xgb',xgb_reg), ('hgb', hgb_reg)], voting='soft', flatten_transform=False)\n",
        "evc_reg2 = VotingClassifier(estimators=[('xgb',xgb_reg), ('hgb', hgb_reg), ('rf', rf)], voting='soft', flatten_transform=False)\n",
        "evc_reg3 = VotingClassifier(estimators=[('xgb',xgb_reg), ('hgb', hgb_reg), ('lgbm', lgbm_reg)], voting='soft', flatten_transform=False)\n",
        "evc_reg4 = VotingClassifier(estimators=[('xgb',xgb_reg), ('hgb', hgb_reg), ('lgbm', lgbm_reg), ('rf', rf)], voting='soft', flatten_transform=False)\n",
        "\n",
        "\n",
        "clfs = {'xgb_reg':xgb_reg, 'lgbm_reg':lgbm_reg, 'hgb_reg':hgb_reg, 'rf':rf, 'evc_reg1': evc_reg1, 'evc_reg2': evc_reg2, 'evc_reg3': evc_reg3, 'evc_reg4': evc_reg4}\n",
        "\n",
        "for clf in clfs:\n",
        "        print(f'{clf}: {fit_predict(clfs[clf], X_train, y_train)} cv {cross_validation(clfs[clf], X_train, y_train, 5)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "random_forest_classification.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
