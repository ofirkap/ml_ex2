{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M52QDmyzhh9s"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open('data/ass2.pickle', 'rb') as handle:\n",
        "    data = pd.read_pickle(handle)\n",
        "\n",
        "X_train, y_train = data['train']\n",
        "X_dev, y_dev = data['dev']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(f'Number of Rows: {len(y_train)}')\n",
        "print(f'The Classes are: {np.unique(y_train)}')\n",
        "print(f'Class 0: {np.count_nonzero(y_train == 0)}')\n",
        "print(f'Class 1: {np.count_nonzero(y_train == 1)}')\n",
        "\n",
        "print(f'Number of Rows: {len(y_dev)}')\n",
        "print(f'The Classes are: {np.unique(y_dev)}')\n",
        "print(f'Class 0: {np.count_nonzero(y_dev == 0)}')\n",
        "print(f'Class 1: {np.count_nonzero(y_dev == 1)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the dataset is unbalanced so we will balance it.\n",
        "We can also see that we are dealing with a binary classification problem so we will be using algorithms that are a better fit for this kind of problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First we run LazyPredict to choose the few best algorithms to continue with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "# clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
        "# models,predictions = clf.fit(X_train, X_dev, y_train, y_dev)\n",
        "\n",
        "# print(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Balance the data in two ways, Over-Sampling and Under-Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "rus = RandomUnderSampler(random_state=42, replacement=True)\n",
        "x_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
        "x_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f'Under sampled: {len(y_rus)}')\n",
        "print(f'Over sampled: {len(y_ros)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our data has 14 features, we might need to reduce some of them based on their importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "def pca(k, X_train):\n",
        "    pca = PCA(n_components=k)\n",
        "    pca.fit(X_train)\n",
        "    return {'train': pd.DataFrame(pca.transform(X_train)), 'test': pd.DataFrame(pca.transform(X_dev))}\n",
        "\n",
        "def select_k_best(k, X_train, y_train):\n",
        "    anova_filter = SelectKBest(score_func=f_regression, k=k)  \n",
        "    anova_filter.fit(X_train, y_train)\n",
        "    return {'train': pd.DataFrame(anova_filter.transform(X_train)), 'test': pd.DataFrame(anova_filter.transform(X_dev))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # import libraries\n",
        "# import seaborn as sns\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# # create pandas df\n",
        "# X = pd.DataFrame(X_train_pca)\n",
        "# y = np.array(y_train)\n",
        "# X['target'] = pd.DataFrame(y.reshape(-1, 1), columns=[\"target\"])\n",
        "# X.head(5)\n",
        "\n",
        "# # check the null values\n",
        "# X.isnull().sum()\n",
        "\n",
        "# # pairplot for distribution\n",
        "# sns.pairplot(X ,hue=\"target\", palette='Set1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def cross_validation(clf, X, y, k):\n",
        "    sc = StandardScaler()\n",
        "    return cross_val_score(clf, sc.fit_transform(X), y, cv=k).mean()\n",
        "\n",
        "def fit_predict(classifier, X_train, y_train):\n",
        "    sc = StandardScaler()\n",
        "    scaled_X_train = sc.fit_transform(X_train)\n",
        "    classifier.fit(scaled_X_train, y_train)\n",
        "\n",
        "    train_score = classifier.score(scaled_X_train, y_train)\n",
        "    dev_score = classifier.score(sc.transform(X_dev), y_dev)\n",
        "    over_fitting = train_score - dev_score\n",
        "\n",
        "    return f'train score: {round(train_score, 3)} test score {round(dev_score, 3)} overfit {round(over_fitting, 3)}'\n",
        "\n",
        "def fit_predict_poly(classifier, X_train, y_train, X_dev, y_dev):\n",
        "    transformed_train = PolynomialFeatures(2).fit_transform(X_train)\n",
        "    transformed_dev = PolynomialFeatures(2).fit_transform(X_dev)\n",
        "    sc = StandardScaler()\n",
        "    scaled_X_train = sc.fit_transform(transformed_train)\n",
        "\n",
        "    classifier.fit(scaled_X_train, y_train)\n",
        "\n",
        "    train_score = classifier.score(scaled_X_train, y_train)\n",
        "    dev_score = classifier.score(sc.transform(transformed_dev), y_dev)\n",
        "    over_fitting = train_score - dev_score\n",
        "\n",
        "    return f'train score: {round(train_score, 3)} test score {round(dev_score, 3)} overfit {round(over_fitting, 3)}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial test of the best algoriths usin cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, VotingClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "def check_models(X_train, y_train, k):\n",
        "    print(f'Random Forest: {cross_validation(RandomForestClassifier(random_state = 42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'SVC: {cross_validation(SVC(random_state = 42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'AdaBoost: {cross_validation(AdaBoostClassifier(random_state=42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'Histogram Gradient Boosting: {cross_validation(HistGradientBoostingClassifier(), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'Bagging Classifier: {cross_validation(BaggingClassifier(base_estimator = SVC(), random_state = 42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'XGBoost: {cross_validation(XGBClassifier(random_state = 42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "    print(f'Light GBM: {cross_validation(LGBMClassifier(random_state = 42), X_train.to_numpy(), y_train.to_numpy(), k)}')\n",
        "\n",
        "print('Undersampled Data:')\n",
        "check_models(x_rus, y_rus, 3)\n",
        "print('Oversampled Data:')\n",
        "check_models(x_ros, y_ros, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find best hyperparameters for each model using RandomizedSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from pprint import pprint\n",
        "\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['log2', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "rf_grid =  {'n_estimators': n_estimators,\n",
        "            'max_features': max_features,\n",
        "            'max_depth': max_depth,\n",
        "            'min_samples_split': min_samples_split,\n",
        "            'min_samples_leaf': min_samples_leaf,\n",
        "            'bootstrap': bootstrap}\n",
        "\n",
        "rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(random_state=42), param_distributions = rf_grid, scoring = 'roc_auc', n_iter = 5, cv = 3, verbose=3, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(x_ros, y_ros)\n",
        "pprint(rf_random.best_params_)\n",
        "print(rf_random.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "svc_grid = {'C': [0.1, 1, 10, 100],\n",
        "            'gamma': [1, 0.1, 0.01, 0.001],\n",
        "            'kernel': ['rbf', 'poly', 'linear']}\n",
        "svc_random = RandomizedSearchCV(estimator = SVC(), param_distributions = svc_grid, scoring = 'roc_auc', n_iter = 5, cv = 3, verbose=3, random_state=42, n_jobs = -1)\n",
        "svc_random.fit(x_ros, y_ros)\n",
        "pprint(svc_random.best_params_)\n",
        "print(svc_random.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "xgb_grid = {\n",
        "    \"learning_rate\"     : [0.05, 0.1, 0.15, 0.2, 0.25, 0.3] ,\n",
        "    \"max_depth\"         : [3, 4, 5, 6, 8, 10, 12, 15],\n",
        "    \"min_child_weight\"  : [1, 3, 5, 7],\n",
        "    \"gamma\"             : [0.0, 0.1, 0.2, 0.3, 0.4],\n",
        "    \"colsample_bytree\"  : [0.3, 0.4, 0.5, 0.7]\n",
        "}\n",
        "\n",
        "xgb_random = RandomizedSearchCV(XGBClassifier(random_state=42), param_distributions=xgb_grid, n_iter=5, scoring='roc_auc', n_jobs=-1, cv=5, verbose=3)\n",
        "xgb_random.fit(x_ros, y_ros)\n",
        "pprint(xgb_random.best_params_)\n",
        "print(xgb_random.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform as sp_uniform\n",
        "\n",
        "lgbm_grid = {'num_leaves': sp_randint(6, 50), \n",
        "             'min_child_samples': sp_randint(100, 500), \n",
        "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
        "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
        "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
        "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
        "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
        "\n",
        "lgbm_random = RandomizedSearchCV(LGBMClassifier(random_state=42), param_distributions=lgbm_grid, n_iter=5, scoring='roc_auc', n_jobs=-1, cv=5, verbose=3)\n",
        "lgbm_random.fit(x_rus, y_rus)\n",
        "pprint(lgbm_random.best_params_)\n",
        "print(lgbm_random.best_estimator_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the new models with the 'dev' data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_ros = XGBClassifier(\n",
        "        base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "        colsample_bynode=1, colsample_bytree=0.4, gamma=0.3, gpu_id=-1,\n",
        "        importance_type='gain', interaction_constraints='',\n",
        "        learning_rate=0.2, max_delta_step=0, max_depth=12,\n",
        "        min_child_weight=3, monotone_constraints='()',\n",
        "        n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=42,\n",
        "        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
        "        tree_method='exact', validate_parameters=1, verbosity=None)\n",
        "\n",
        "xgb_rus = XGBClassifier(\n",
        "        base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "        colsample_bynode=1, colsample_bytree=0.5, gamma=0.3, gpu_id=-1,\n",
        "        importance_type='gain', interaction_constraints='',\n",
        "        learning_rate=0.15, max_delta_step=0, max_depth=5,\n",
        "        min_child_weight=1, monotone_constraints='()',\n",
        "        n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=42,\n",
        "        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
        "        tree_method='exact', validate_parameters=1, verbosity=None)\n",
        "\n",
        "lgbm_ros = LGBMClassifier(\n",
        "        colsample_bytree=0.7077667509501917, min_child_samples=424,\n",
        "        min_child_weight=1e-05, num_leaves=33, random_state=42,\n",
        "        reg_alpha=2, reg_lambda=10, subsample=0.366858596641466)\n",
        "\n",
        "lgbm_rus = LGBMClassifier(\n",
        "        colsample_bytree=0.41365111596542925, min_child_samples=347,\n",
        "        min_child_weight=0.1, num_leaves=33, random_state=42,\n",
        "        reg_alpha=0.1, reg_lambda=1, subsample=0.6512961625220872)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=120)\n",
        "\n",
        "evc = VotingClassifier(estimators=[('xgb',xgb_ros), ('rf', rf)],voting='hard')\n",
        "\n",
        "clfs = {'xgb_ros':xgb_ros, 'xgb_rus':xgb_rus, 'lgbm_ros':lgbm_ros, 'lgbm_rus':lgbm_rus, 'rf':rf, 'evc': evc}\n",
        "\n",
        "best_X_train, best_X_dev = select_k_best(5, x_ros, y_ros).values()\n",
        "pca_X_train, pca_X_dev = pca(5, x_ros).values()\n",
        "\n",
        "for clf in clfs:\n",
        "        print(f'{clf}: {fit_predict(clfs[clf], x_ros, y_ros)}')\n",
        "        print(f'poly {clf}: {fit_predict_poly(clfs[clf], x_ros, y_ros, X_dev, y_dev)}')\n",
        "        print(f'poly k best {clf}: {fit_predict_poly(clfs[clf], best_X_train, y_ros, best_X_dev, y_dev)}')\n",
        "        print(f'poly pca {clf}: {fit_predict_poly(clfs[clf], pca_X_train, y_ros, pca_X_dev, y_dev)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "random_forest_classification.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
